{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09b79db3",
   "metadata": {},
   "source": [
    "Open question 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18707ab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## Detailed Architecture Flow\n",
    "\n",
    "1) Object Storage (S3 Bucket):\n",
    "\n",
    "    Purpose: Store uploaded images.\n",
    "\n",
    "    Setup:\n",
    "\n",
    "    -Create an S3 bucket (e.g., image-upload-bucket).\n",
    "\n",
    "    -Configure S3 Event Notifications to send events to an SQS queue for each new image.\n",
    "\n",
    "2) Batch Trigger (SQS Queue):\n",
    "\n",
    "    Purpose: Buffer image events and trigger processing when 20 images accumulate.\n",
    "\n",
    "    Setup:\n",
    "\n",
    "    -Create a Standard SQS Queue (e.g., image-batch-queue).\n",
    "\n",
    "    -Configure S3 Event Notifications to send events to this queue.\n",
    "\n",
    "    -Use CloudWatch Alarm to monitor queue depth:\n",
    "\n",
    "    -Metric: ApproximateNumberOfMessagesVisible\n",
    "\n",
    "    -Alarm Condition: >= 20 images\n",
    "\n",
    "    -Attach the alarm to an AWS Lambda function (Batch Processor) via EventBridge.\n",
    "\n",
    "3) Batch Processor (Lambda 1):\n",
    "\n",
    "    Purpose: Group 20 images into a batch and trigger processing.\n",
    "\n",
    "    Setup:\n",
    "\n",
    "    -Using the custom container image of this repo (i might use ECR to store the image)\n",
    "\n",
    "    Logic:\n",
    "\n",
    "    -Fetch messages from SQS (up to 20 per invocation).\n",
    "\n",
    "    -Extract S3 object keys from messages.\n",
    "\n",
    "    -Invoke the Processor Lambda with the batch of 100 keys.\n",
    "\n",
    "    -Delete processed messages from SQS.\n",
    "\n",
    "    -IAM Role: Access to SQS, Lambda invocation, and S3 read.\n",
    "\n",
    "4) Processor (Lambda 2):\n",
    "\n",
    "    Purpose: Process images, save results to DB, and notify users.\n",
    "\n",
    "    Logic:\n",
    "\n",
    "    Request 20 images from S3.\n",
    "\n",
    "    Process images (This will require adding a image loader addition to the container).\n",
    "\n",
    "    Write results to DynamoDB. (This will require adding writing to dynamo from the container)\n",
    "\n",
    "    Publish success message to SNS.\n",
    "\n",
    "    IAM Role: Access to S3, DynamoDB, and SNS.\n",
    "\n",
    "5) Database (DynamoDB):\n",
    "\n",
    "    Purpose: Store processed image metadata and batch info.\n",
    "\n",
    "    Table Design:\n",
    "\n",
    "        Table 1: ProcessedBatches (Batch-level metadata)\n",
    "\n",
    "        batch_id\n",
    "\n",
    "        image_count (e.g., 20)\n",
    "\n",
    "        status (COMPLETED or ERROR)\n",
    "\n",
    "        processed_at (timestamp)\n",
    "\n",
    "        stats (the dictionnary of stats)\n",
    "\n",
    "6) User Notification (SNS):\n",
    "\n",
    "    Purpose: Alert end-users upon batch completion.\n",
    "\n",
    "    Setup:\n",
    "\n",
    "    -Create an SNS topic (e.g., batch-completion-topic).\n",
    "\n",
    "    -Subscribe users (email/SMS/HTTP).\n",
    "\n",
    "    -Processor Lambda publishes to this topic after DynamoDB write.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
